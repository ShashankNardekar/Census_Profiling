{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c2bae-3521-4a70-9b66-6424f7b682db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_NAs(df,val):\n",
    "\n",
    "    # strip leading and trailing whitespace in columns\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    \n",
    "    nan_perc = (df.isnull().sum() / len(df)) * 100\n",
    "    nan_df = nan_perc.to_frame().reset_index()\n",
    "    if len(nan_df[nan_df[0]>0]) > 0:\n",
    "        print('\\nNAs present in the columns of ',val,':')\n",
    "        nan_df1 = nan_df[nan_df[0]>0].sort_values(0,ascending=False).rename(columns={'index':'column',0:'%'})\n",
    "        nan_df1['%'] = nan_df1['%'].round(2)\n",
    "        print(nan_df1)\n",
    "    else:\n",
    "        print('\\nNo NAs present in the columns of ',val)\n",
    "\n",
    "    nan_perc = (df.eq('').sum() / len(df)) * 100\n",
    "    nan_df = nan_perc.to_frame().reset_index()\n",
    "    if len(nan_df[nan_df[0]>0]) > 0:\n",
    "        print('\\nempty values present in the columns of ',val,':')\n",
    "        nan_df1 = nan_df[nan_df[0]>0].sort_values(0,ascending=False).rename(columns={'index':'column',0:'%'})\n",
    "        nan_df1['%'] = nan_df1['%'].round(2)\n",
    "        print(nan_df1)\n",
    "    else:\n",
    "        print('\\nNo empty values present in the columns of ',val)\n",
    "\n",
    "    nan_perc = (df.eq(0).sum() / len(df)) * 100\n",
    "    nan_df = nan_perc.to_frame().reset_index()\n",
    "    if len(nan_df[nan_df[0]>0]) > 0:\n",
    "        print('\\n0 values present in the columns of ',val,':')\n",
    "        nan_df1 = nan_df[nan_df[0]>0].sort_values(0,ascending=False).rename(columns={'index':'column',0:'%'})\n",
    "        nan_df1['%'] = nan_df1['%'].round(2)\n",
    "        print(nan_df1)\n",
    "    else:\n",
    "        print('\\nNo 0 values present in the columns of ',val)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_data(file):\n",
    "    return (\n",
    "        \n",
    "        pd.read_csv(file,header=None) \n",
    "        .rename(columns={k : col_dict[k][0] for k in col_dict})\n",
    "        .drop('instance_weight_ignore',axis=1)    \n",
    "    )\n",
    "\n",
    "def anomalies(df):\n",
    "    \n",
    "    anomaly_workers = df[((df['wage_per_hour']>0)|\n",
    "     (df['tax_filer_status']!='Nonfiler')|(df['num_persons_worked_for_employer']>0)|(df['weeks_worked_in_year']>0))\n",
    "     &(df['age']<13)][['age','wage_per_hour','tax_filer_status','num_persons_worked_for_employer'\n",
    "                              ,'weeks_worked_in_year','target']].drop_duplicates()\n",
    "    \n",
    "    \n",
    "    anomaly_married = df[(df['marital_status']!='Never married')&(df['age']<15)][['age','marital_status','target']].drop_duplicates()\n",
    "\n",
    "    return anomaly_workers,anomaly_married\n",
    "\n",
    "def remove_anomalies(orig_df,anom_df):\n",
    "    orig_df = orig_df[~(orig_df.index).isin(anom_df.index)]\n",
    "    orig_df = orig_df.reset_index(drop=True)\n",
    "    return orig_df\n",
    "\n",
    "def ft_pt_func(df):\n",
    "\n",
    "    ft_pt = ['Full-time schedules', 'PT for econ reasons usually PT', 'PT for non-econ reasons usually FT']\n",
    "    arm_ch_pt = ['Children or Armed Forces', 'PT for econ reasons usually FT']\n",
    "    unemp = ['Unemployed full-time', 'Unemployed part- time', 'Not in labor force']\n",
    "    \n",
    "    df['ft_pt_group'] = ''\n",
    "    df.loc[df['full_or_part_time_employment_stat'].isin(ft_pt),'ft_pt_group'] = 'full_part'\n",
    "    df.loc[df['full_or_part_time_employment_stat'].isin(arm_ch_pt),'ft_pt_group'] = 'arm_child_partt'\n",
    "    df.loc[df['full_or_part_time_employment_stat'].isin(unemp),'ft_pt_group'] = 'unemp'\n",
    "    \n",
    "    # print(df['education_group'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['full_or_part_time_employment_stat']\n",
    "\n",
    "    return df\n",
    "    \n",
    "def mig_msa(df):\n",
    "\n",
    "    non_mov_q = ['?', 'Nonmover']\n",
    "    to_msa_nonmsa = ['MSA to MSA', 'NonMSA to MSA', 'Not identifiable', 'Abroad to MSA', 'MSA to nonMSA']\n",
    "    to_nonmsa = ['NonMSA to nonMSA', 'Abroad to nonMSA', 'Not in universe']\n",
    "\n",
    "    df['mig_msa_group'] = ''\n",
    "    df.loc[df['migration_code_change_in_msa'].isin(non_mov_q),'mig_msa_group'] = 'non_mover'\n",
    "    df.loc[df['migration_code_change_in_msa'].isin(to_msa_nonmsa),'mig_msa_group'] = 'to_msa_nonmsa'\n",
    "    df.loc[df['migration_code_change_in_msa'].isin(to_nonmsa),'mig_msa_group'] = 'to_nonmsa'\n",
    "    \n",
    "    # print(df['education_group'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['migration_code_change_in_msa']\n",
    "\n",
    "    return df\n",
    "    \n",
    "def age_bucket(df):\n",
    "    # len(df_train_a[df_train_a['age']<=0])\n",
    "    df['age_bucket'] = ''\n",
    "    df.loc[(df['age']<=24),'age_bucket'] = '0_24'\n",
    "    df.loc[(df['age']>=25)&(df['age']<=37),'age_bucket'] = '25_37'\n",
    "    df.loc[(df['age']>=38)&(df['age']<=52),'age_bucket'] = '38_52'\n",
    "    df.loc[(df['age']>=53),'age_bucket'] = '53+'\n",
    "    \n",
    "    # df_train_a['age_bucket'].value_counts(normalize=True)\n",
    "    \n",
    "    del df['age']\n",
    "\n",
    "    return df\n",
    "\n",
    "def weeks_bucket(df):\n",
    "    # len(df_train_a[df_train_a['age']<=0])\n",
    "    df['weeks_bucket'] = ''\n",
    "    df.loc[(df['weeks_worked_in_year']<=0),'weeks_bucket'] = '0'\n",
    "    df.loc[(df['weeks_worked_in_year']>=1)&(df['weeks_worked_in_year']<=26),'weeks_bucket'] = '1_26'\n",
    "    df.loc[(df['weeks_worked_in_year']>=27)&(df['weeks_worked_in_year']<=39),'weeks_bucket'] = '27_39'\n",
    "    df.loc[(df['weeks_worked_in_year']>=40)&(df['weeks_worked_in_year']<=51),'weeks_bucket'] = '40_51'\n",
    "    df.loc[(df['weeks_worked_in_year']>=52),'weeks_bucket'] = '52+'\n",
    "    \n",
    "    # df_train_a['age_bucket'].value_counts(normalize=True)\n",
    "    \n",
    "    del df['weeks_worked_in_year']\n",
    "\n",
    "    return df\n",
    "\n",
    "def wage_bucket(df):\n",
    "    # len(df_train_a[df_train_a['age']<=0])\n",
    "    df['wage_bucket'] = ''\n",
    "    df.loc[(df['wage_per_hour']<=0),'wage_bucket'] = '0'\n",
    "    df.loc[(df['wage_per_hour']>0),'wage_bucket'] = '1'\n",
    "    \n",
    "    # df_train_a['age_bucket'].value_counts(normalize=True)\n",
    "    \n",
    "    del df['wage_per_hour']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def education_bucket(df):\n",
    "    df['education'] = df['education'].str.strip()\n",
    "\n",
    "    \n",
    "    profess_degree = ['Prof school degree (MD DDS DVM LLB JD)','Doctorate degree(PhD EdD)']\n",
    "    masters_degree = ['Masters degree(MA MS MEng MEd MSW MBA)']\n",
    "    bachelors_degree = ['Bachelors degree(BA AB BS)']\n",
    "    assoc_grad = ['Associates degree-academic program', 'Associates degree-occup /vocational', 'Some college but no degree', 'High school graduate']\n",
    "    less_than_12th = ['12th grade no diploma', '7th and 8th grade', '11th grade', '10th grade', '9th grade', '1st 2nd 3rd or 4th grade', '5th or 6th grade', 'Less than 1st grade', 'Children']\n",
    "\n",
    "    \n",
    "    \n",
    "    df['education_group'] = ''\n",
    "    df.loc[df['education'].isin(profess_degree),'education_group'] = 'profess'\n",
    "    df.loc[df['education'].isin(masters_degree),'education_group'] = 'masters'\n",
    "    df.loc[df['education'].isin(bachelors_degree),'education_group'] = 'bachelors'\n",
    "    df.loc[df['education'].isin(assoc_grad),'education_group'] = 'assoc_grd'\n",
    "    df.loc[df['education'].isin(less_than_12th),'education_group'] = '<_12th'\n",
    "    \n",
    "    # df.loc[df['education'].isin(degree),'education_group'] = 'degree'\n",
    "    # df.loc[~df['education'].isin(degree),'education_group'] = 'non_degree'\n",
    "    \n",
    "    # print(df['education_group'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['education']\n",
    "\n",
    "    return df\n",
    "\n",
    "def married_bucket(df):\n",
    "    \n",
    "    df['marital_status'] = df['marital_status'].str.strip()\n",
    "    \n",
    "    married_spouse_present = ['Married-civilian spouse present']\n",
    "    divorced = ['Divorced']\n",
    "    married_spouse_abs = ['Married-spouse absent']\n",
    "    sep_wid = ['Separated', 'Widowed']\n",
    "    never_af = ['Never married', 'Married-A F spouse present']    \n",
    "    \n",
    "    df['married_or_not'] = ''\n",
    "    df.loc[df['marital_status'].isin(married_spouse_present),'married_or_not'] = 'M_spouse_pr'\n",
    "    df.loc[df['marital_status'].isin(divorced),'married_or_not'] = 'divorced'\n",
    "    df.loc[df['marital_status'].isin(married_spouse_abs),'married_or_not'] = 'M_spouse_ab'\n",
    "    df.loc[df['marital_status'].isin(sep_wid),'married_or_not'] = 'sep_wid'\n",
    "    df.loc[df['marital_status'].isin(never_af),'married_or_not'] = 'never_M_AF'\n",
    "    \n",
    "    # print(df['married_or_not'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['marital_status']\n",
    "\n",
    "    return df\n",
    "\n",
    "def race_bucket(df):\n",
    "    df['race'] = df['race'].str.strip()\n",
    "    \n",
    "    df['white_or_nonwhite'] = ''\n",
    "    df.loc[df['race'].isin(['White']),'white_or_nonwhite'] = 'white'\n",
    "    df.loc[~df['race'].isin(['White']),'white_or_nonwhite'] = 'non_white'\n",
    "    \n",
    "    # print(df['white_or_nonwhite'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['race']\n",
    "\n",
    "    return df\n",
    "\n",
    "def hispanic_bucket(df):\n",
    "    # print(df_train_a['hispanic_Origin'].value_counts(normalize=True))\n",
    "    \n",
    "    df['hispanic_Origin'] = df['hispanic_Origin'].str.strip()\n",
    "    \n",
    "    other = [ 'Do not know','NA','All other']\n",
    "    hispanic = ['Central or South American','Mexican (Mexicano)', 'Mexican-American', 'Other Spanish','Puerto Rican', 'Cuban', 'Chicano']\n",
    "    \n",
    "    df['hispanic_or_other'] = ''\n",
    "    df.loc[df['hispanic_Origin'].isin(other),'hispanic_or_other'] = 'other'\n",
    "    df.loc[df['hispanic_Origin'].isin(hispanic),'hispanic_or_other'] = 'hispanic'\n",
    "    \n",
    "    # print(df['hispanic_or_other'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['hispanic_Origin']\n",
    "    return df\n",
    "\n",
    "def birth_country_bucket(df):\n",
    "    df['country_of_birth_father'] = df['country_of_birth_father'].str.strip()\n",
    "    df['country_of_birth_mother'] = df['country_of_birth_mother'].str.strip()\n",
    "    df['country_of_birth_self'] = df['country_of_birth_self'].str.strip()\n",
    "    \n",
    "    df['immigrant_parents'] = ''\n",
    "    df.loc[(df['country_of_birth_father']=='United-States')|\n",
    "                    (df['country_of_birth_mother']=='United-States'),'immigrant_parents'] = 'no'\n",
    "    \n",
    "    df.loc[(df['country_of_birth_father']!='United-States')&\n",
    "                    (df['country_of_birth_mother']!='United-States'),'immigrant_parents'] = 'yes'\n",
    "    \n",
    "    # print(df['immigrant_parents'].value_counts(normalize=True))\n",
    "    \n",
    "    df['immigrant'] = ''\n",
    "    df.loc[(df['country_of_birth_self']=='United-States'),'immigrant'] = 'no'\n",
    "    df.loc[(df['country_of_birth_self']!='United-States'),'immigrant'] = 'yes'\n",
    "    \n",
    "    # print(df['immigrant'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['country_of_birth_father']\n",
    "    del df['country_of_birth_mother']\n",
    "    del df['country_of_birth_self']\n",
    "    return df\n",
    "\n",
    "def citizen_bucket(df):\n",
    "    # df_train_a['citizenship'].value_counts(normalize=True)\n",
    "    df['citizenship'] = df['citizenship'].str.strip()\n",
    "    \n",
    "    df['us_citizen'] = ''\n",
    "    df.loc[(df['citizenship']=='Foreign born- Not a citizen of U S'),'us_citizen'] = 'no'\n",
    "    df.loc[(df['citizenship']!='Foreign born- Not a citizen of U S'),'us_citizen'] = 'yes'\n",
    "    \n",
    "    # print(df['us_citizen'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['citizenship']\n",
    "    return df\n",
    "\n",
    "def capital_net(df):\n",
    "    df['capital_net'] = df['capital_gains'] - df['capital_losses'] + df['divdends_from_stocks']\n",
    "    \n",
    "    del df['capital_gains']\n",
    "    del df['capital_losses']\n",
    "    del df['divdends_from_stocks']\n",
    "    return df\n",
    "\n",
    "def capital(df):\n",
    "    #df['capital_net'] = df['capital_gains'] - df['capital_losses'] + df['divdends_from_stocks']\n",
    "    \n",
    "    df['capital_gains_bucket'] = ''\n",
    "    df['capital_losses_bucket'] = ''\n",
    "    df['divdends_from_stocks_bucket'] = ''\n",
    "    df.loc[(df['capital_gains']>0),'capital_gains_bucket'] = 1\n",
    "    df.loc[(df['capital_gains']<=0),'capital_gains_bucket'] = 0\n",
    "    df.loc[(df['capital_losses']>0),'capital_losses_bucket'] = 1\n",
    "    df.loc[(df['capital_losses']<=0),'capital_losses_bucket'] = 0\n",
    "    df.loc[(df['divdends_from_stocks']>0),'divdends_from_stocks_bucket'] = 1\n",
    "    df.loc[(df['divdends_from_stocks']<=0),'divdends_from_stocks_bucket'] = 0\n",
    "    # print(df['us_citizen'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['capital_gains']\n",
    "    del df['capital_losses']\n",
    "    del df['divdends_from_stocks']\n",
    "    return df\n",
    "\n",
    "def worker_bucket(df):\n",
    "    df['class_of_worker'] = df['class_of_worker'].str.strip()\n",
    "\n",
    "    self_inc = ['Self-employed-incorporated']\n",
    "    fed = ['Federal government']\n",
    "    self_gov_priv = ['Self-employed-not incorporated', 'State government', 'Local government', 'Private']\n",
    "    na_wo = ['Not in universe', 'Without pay', 'Never worked']\n",
    "    \n",
    "    df['worker_class'] = ''\n",
    "    df.loc[(df['class_of_worker'].isin(self_inc)),'worker_class'] = 'self_incrp'\n",
    "    df.loc[(df['class_of_worker'].isin(fed)),'worker_class'] = 'federal'\n",
    "    df.loc[(df['class_of_worker'].isin(self_gov_priv)),'worker_class'] = 'self_gov_priv'\n",
    "    df.loc[(df['class_of_worker'].isin(na_wo)),'worker_class'] = 'na_wo_pay'\n",
    "    \n",
    "    \n",
    "    # print(df['worker_class'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['class_of_worker']\n",
    "\n",
    "    return df\n",
    "\n",
    "def householder(df):\n",
    "    df['detailed_household_summary_in_household'] = df['detailed_household_summary_in_household'].str.strip()\n",
    "    \n",
    "    householder = ['Householder']\n",
    "    spouse_nonrel = ['Spouse of householder', 'Nonrelative of householder']\n",
    "    other = ['Other relative of householder', 'Child 18 or older', 'Group Quarters- Secondary individual',\n",
    "        'Child under 18 never married', 'Child under 18 ever married']\n",
    "\n",
    "    df['household'] = ''\n",
    "    df.loc[(df['detailed_household_summary_in_household'].isin(householder)),'household'] = 'HH'\n",
    "    df.loc[(df['detailed_household_summary_in_household'].isin(spouse_nonrel)),'household'] = 'spouse_norel'\n",
    "    df.loc[(df['detailed_household_summary_in_household'].isin(other)),'household'] = 'other'\n",
    "    \n",
    "    \n",
    "    # print(df['household'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['detailed_household_summary_in_household']\n",
    "    return df\n",
    "\n",
    "def tax(df):\n",
    "    df['tax_filer_status'] = df['tax_filer_status'].str.strip()\n",
    "    \n",
    "    joint_both_under = ['Joint both under 65']\n",
    "    rest = ['Joint one under 65 & one 65+', 'Joint both 65+', 'Single', 'Head of household']\n",
    "    non = ['Nonfiler']\n",
    "\n",
    "    df['tax'] = ''\n",
    "    df.loc[(df['tax_filer_status'].isin(joint_both_under)),'tax'] = 'joint_both_under'\n",
    "    df.loc[(df['tax_filer_status'].isin(rest)),'tax'] = 'rest'\n",
    "    df.loc[(df['tax_filer_status'].isin(non)),'tax'] = 'non_filer'\n",
    "    \n",
    "    \n",
    "    # print(df['tax_filer_status'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['tax_filer_status']\n",
    "    return df\n",
    "\n",
    "def occupation(df):\n",
    "    df['major_occupation_code'] = df['major_occupation_code'].str.strip()\n",
    "\n",
    "    exec_arm = ['Executive admin and managerial', 'Professional specialty', 'Armed Forces']\n",
    "    group1 = ['Protective services', 'Sales', 'Technicians and related support', 'Precision production craft & repair']\n",
    "    group2 = ['Transportation and material moving', 'Farming forestry and fishing', 'Machine operators assmblrs & inspctrs',\n",
    "        'Adm support including clerical', 'Handlers equip cleaners etc', 'Not in universe', 'Other service', 'Private household services']\n",
    "    \n",
    "    df['prof'] = ''\n",
    "    df.loc[(df['major_occupation_code'].isin(exec_arm)),'prof'] = 'exec_army'\n",
    "    df.loc[(df['major_occupation_code'].isin(group1)),'prof'] = 'sales_tech_protect'\n",
    "    df.loc[(df['major_occupation_code'].isin(group2)),'prof'] = 'primary'\n",
    "    \n",
    "    # print(df['major_occupation_code'].value_counts(normalize=True))\n",
    "    \n",
    "    del df['major_occupation_code']\n",
    "    return df\n",
    "\n",
    "def pie_plot(col):\n",
    "    df = df_train_a.groupby([col,target_col],as_index=False).size()\n",
    "    \n",
    "    # Calculate the total size for each worker class\n",
    "    total_size = df.groupby(col)['size'].sum()\n",
    "    \n",
    "    # Create a pivot table to separate target 0 and target 1\n",
    "    pivot_table = df.pivot(index=col, columns='target', values='size')\n",
    "    \n",
    "    # Plot the pie charts\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Pie chart 1: Total distribution of worker classes\n",
    "    axes[0].pie(total_size, labels=None, autopct='%1.1f%%', startangle=90,colors=sns.color_palette('Set3'), pctdistance=0.85)\n",
    "    axes[0].set_title('Total Distribution')\n",
    "    \n",
    "    # Pie chart 2: Distribution of worker classes in target 0\n",
    "    axes[1].pie(pivot_table[0], labels=None, autopct='%1.1f%%', startangle=90,colors=sns.color_palette('Set3'), pctdistance=0.85)\n",
    "    axes[1].set_title('Distribution of < $50k income')\n",
    "    \n",
    "    # Pie chart 3: Distribution of worker classes in target 1\n",
    "    axes[2].pie(pivot_table[1], labels=None, autopct='%1.1f%%', startangle=90,colors=sns.color_palette('Set3'), pctdistance=0.85)\n",
    "    axes[2].set_title('Distribution of > $50k income')\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend(title=col, loc='upper right', labels=total_size.index)\n",
    "\n",
    "    #axes[0].legend(total_size.index, title=col, loc='upper right')\n",
    "    #plt.title(col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def pivot_grp(df,col):\n",
    "    total_size = df.groupby([col,target_col]).size().reset_index().rename(columns={0:'size'})\n",
    "    \n",
    "    # Create a pivot table to separate target 0 and target 1\n",
    "    pivot_table = total_size.pivot(index=col, columns='target', values='size')\n",
    "    piv = pivot_table.reset_index()\n",
    "    piv['0%'] = (piv[0]*100  / piv[0].sum()).round(2)\n",
    "    piv['1%'] = (piv[1]*100 / piv[1].sum()).round(2)\n",
    "    piv['%difference'] = (piv['1%']) - (piv['0%'] )\n",
    "    return piv[[col,'0%','1%','%difference']].sort_values(by=['%difference'],ascending=False)\n",
    "\n",
    "def hyper_lr(X,y):\n",
    "    solvers = ['lbfgs','newton-cg','liblinear','sag','saga']\n",
    "    penalty = ['l1', 'l2', 'elasticnet', 'none']\n",
    "    C = np.logspace(-4, 4, 20)\n",
    "    \n",
    "    # Define compatible pairs\n",
    "    compatible_pairs = {\n",
    "        'liblinear': ['l1', 'l2'],\n",
    "        'saga': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'lbfgs': ['l2', 'none'],\n",
    "        'newton-cg': ['l2', 'none'],\n",
    "        'sag': ['l2', 'none']\n",
    "    }\n",
    "    \n",
    "    # Create a new grid ensuring compatibility\n",
    "    new_grid = []\n",
    "    for solver in solvers:\n",
    "        for pen in compatible_pairs[solver]:\n",
    "            for c in C:\n",
    "                new_grid.append({'solver': [solver], 'penalty': [pen], 'C': [c]})\n",
    "    \n",
    "    \n",
    "    before = time.time()\n",
    "    \n",
    "    clf2 = LogisticRegression()\n",
    "    \n",
    "    # define grid search\n",
    "    # grid = dict(solver=solvers,penalty=penalty,C=C)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "    grid_search = RandomizedSearchCV(estimator=clf2, param_distributions=new_grid, n_jobs=-1, cv=cv, scoring='f1_macro',error_score=0)\n",
    "    # grid_result = grid_search.fit(X_train_scaled, y_train)\n",
    "    grid_result = grid_search.fit(X, y)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    \n",
    "    after = time.time()\n",
    "    print(after-before,'seconds')\n",
    "    \n",
    "    return grid_result\n",
    "    \n",
    "def lr_report(grid_result,X,y):\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    clf3 = LogisticRegression(**best_params).fit(X,y)\n",
    "    y_pred2 = clf3.predict(X_test_scaled)\n",
    "    print(classification_report(y_test, y_pred2))\n",
    "    return clf3\n",
    "\n",
    "def save_model(model,path):\n",
    "    # Save the trained model to a file\n",
    "    joblib.dump(model, path)\n",
    "\n",
    "def read_model(path):\n",
    "    # Load the saved model from the file\n",
    "    model = joblib.load(path)\n",
    "    return model\n",
    "\n",
    "def hyper_rf(X,y,n_splits,n_repeats,class_weight):\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    before = time.time()\n",
    "    \n",
    "    param_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [5,20,30],\n",
    "        'max_features': ['log2', 'sqrt'],\n",
    "        'min_samples_leaf': [3, 5],\n",
    "        'min_samples_split': [8, 10,20],\n",
    "        'n_estimators': [50,100,20]\n",
    "    }\n",
    "    \n",
    "    rfc = RandomForestClassifier(class_weight=class_weight)\n",
    "    # Create a based model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n",
    "    # grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
    "    random_search = RandomizedSearchCV(estimator=rfc, param_distributions = param_grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
    "    \n",
    "    # 'average_precision', 'precision_samples','roc_auc_ovo','roc_auc','roc_auc_ovr_weighted','precision_weighted','recall_macro',\n",
    "    # 'f1_macro','balanced_accuracy','f1_samples'\n",
    "    \n",
    "    random_result = random_search.fit(X, y)\n",
    "    print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))\n",
    "    after = time.time()\n",
    "    print(after-before,'seconds')\n",
    "\n",
    "    return random_result\n",
    "\n",
    "def rf_report(random_result,X,y):\n",
    "    # Assuming random_result is the result from RandomizedSearchCV\n",
    "    best_params = random_result.best_params_\n",
    "    \n",
    "    # Now use these best parameters to create a new RandomForestClassifier\n",
    "    rf = RandomForestClassifier(**best_params)\n",
    "    \n",
    "    # rf.fit(X, y)\n",
    "    # rf.fit(X_train, y_train)\n",
    "    rf.fit(X, y)\n",
    "    # y_pred4 = rf.predict(X_test_scaled)\n",
    "    y_pred4 = rf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred4))\n",
    "\n",
    "    return rf\n",
    "\n",
    "def calculate_shap_values(model, X):\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    return shap_values\n",
    "\n",
    "def plot_shap_values(shap_values, feature_names):\n",
    "    # Create the beeswarm plot\n",
    "    plt.figure(figsize=(6, 3)) # changing the figure size here is not reflecting on the graph\n",
    "    shap.summary_plot(shap_values, feature_names=feature_names, plot_type='bar')\n",
    "    plt.show()\n",
    "\n",
    "def hyper_lgb(X,y):\n",
    "    before = time.time()\n",
    "    # Define the LightGBM classifier\n",
    "    # lgbm_classifier = lgb.LGBMClassifier(objective='binary', random_state=42,is_unbalance=True)\n",
    "    lgbm_classifier = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
    "    \n",
    "    # Update the pipeline with LightGBM classifier\n",
    "    lgbm_pipeline = Pipeline([\n",
    "        ('classifier', lgbm_classifier)\n",
    "    ])\n",
    "    \n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20],\n",
    "        'min_child_samples' : [5, 10, 20]\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "    #grid_search_lgbm = GridSearchCV(lgbm_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "    random_search_lgbm = RandomizedSearchCV(estimator=lgbm_classifier, param_distributions = param_grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0)\n",
    "    \n",
    "    # Fit the grid search model\n",
    "    # grid_search_lgbm.fit(X_train, y_train)\n",
    "    random_search_lgbm.fit(X, y)\n",
    "    \n",
    "    # Best parameters and F1 score\n",
    "    best_params_lgbm = random_search_lgbm.best_params_\n",
    "    best_f1_score_lgbm = random_search_lgbm.best_score_\n",
    "    \n",
    "    print(\"Best Parameters:\", best_params_lgbm)\n",
    "    print(\"Best F1 Score:\", best_f1_score_lgbm)\n",
    "    \n",
    "    after = time.time()\n",
    "    \n",
    "    print('total time:',after-before)\n",
    "    return best_params_lgbm\n",
    "\n",
    "def lgb_report(X,y,best_params_lgbm):\n",
    "    lgbm_classifier = lgb.LGBMClassifier(**best_params_lgbm)\n",
    "    \n",
    "    lgbm_classifier.fit(X, y)\n",
    "    \n",
    "    y_pred6 = lgbm_classifier.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred6))\n",
    "    return lgbm_classifier\n",
    "\n",
    "\n",
    "def hyper_xgb(X,y):\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators':[50,100,200],\n",
    "        'min_child_weight':[1, 3, 7],\n",
    "        'subsample':[0.6, 0.8, 1.0],\n",
    "        'max_depth': [4, 8, 10, 20]\n",
    "    }\n",
    "    \n",
    "    # label_encoder = LabelEncoder()\n",
    "    # y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    \n",
    "    # Create a based model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='precision',error_score=0)\n",
    "    # random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions = param_grid, n_jobs=-1, cv=cv, scoring='f1_weighted',error_score=0)\n",
    "    \n",
    "    grid_search_result = grid_search.fit(X, y)\n",
    "    print(\"Best: %f using %s\" % (grid_search_result.best_score_, grid_search_result.best_params_))\n",
    "    xgb_best_params = grid_search_result.best_params_\n",
    "    \n",
    "    return xgb_best_params\n",
    "\n",
    "def xgb_report(X,y,xgb_best_params):\n",
    "    xgb_hyper = xgb.XGBClassifier(**xgb_best_params)\n",
    "    xgb_hyper.fit(X, y)\n",
    "    \n",
    "    \n",
    "    y_pred_xgb= xgb_hyper.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "    \n",
    "    return xgb_hyper\n",
    "\n",
    "def majority_vote(y_pred_lr, y_pred_rf, y_pred_lgbm,y_pred_xgm,y_auto_preds):\n",
    "    \n",
    "    # if len(roc_predictions) != len(lgbm_predictions) or len(roc_predictions) != len(xgb_predictions):\n",
    "    #     raise ValueError(\"All input arrays must have the same length\")\n",
    "\n",
    "    majority_array = []\n",
    "    for lr1, rf1, lgbm1, xgb1, auto1 in zip(y_pred_lr, y_pred_rf, y_pred_lgbm,y_pred_xgm,y_auto_preds):\n",
    "        \n",
    "        count_ones = sum([lr1, rf1, lgbm1, xgb1, auto1])\n",
    "        \n",
    "        majority_array.append(1 if count_ones >= 3 else 0)\n",
    "\n",
    "    return majority_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
